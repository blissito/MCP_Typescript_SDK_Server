# MCP Server Blissmo experiment üëΩ

Un servidor simple de Model Context Protocol (MCP) creado con TypeScript con soporte para LLMs REST API.

## üéØ ¬øQu√© es esto?

Este proyecto demuestra c√≥mo conectar un **LLM REST API** (como OpenAI, Claude, Ollama, etc.) con un **servidor MCP** para darle "superpoderes" al LLM, permiti√©ndole acceder a recursos y ejecutar herramientas en tu sistema.

## üèóÔ∏è Arquitectura

```
Usuario ‚Üí LLM REST API ‚Üí LLMRestClient ‚Üí MCP Server ‚Üí Recursos/Herramientas
```

### Flujo de trabajo:

1. **Usuario** hace una consulta al LLM
2. **LLM** analiza qu√© necesita hacer
3. **LLMRestClient** ejecuta las acciones en el servidor MCP
4. **MCP Server** lee recursos o ejecuta herramientas
5. **LLM** genera una respuesta final con los resultados

## üéØ Casos de Uso

### 1. **Gesti√≥n de Infraestructura IT**

```bash
Usuario: "Verifica el estado de los servidores de producci√≥n y ejecuta el backup autom√°tico"
LLM: [Lee m√©tricas] [Ejecuta backup] "Los 12 servidores est√°n operativos al 99.2% de rendimiento. He iniciado el backup incremental que completar√° en 15 minutos, liberando 2.3GB de espacio."
```

### 2. **An√°lisis Financiero y Reportes**

```bash
Usuario: "Lee el reporte de ventas del Q4 y ejecuta el an√°lisis de rentabilidad por regi√≥n"
LLM: [Lee datos] [Ejecuta an√°lisis] "Las ventas del Q4 alcanzaron $2.4M, un incremento del 18% vs Q3. La regi√≥n Norte muestra la mayor rentabilidad (34%), mientras que el Sur requiere atenci√≥n con un margen del 12%."
```

### 3. **Monitoreo de Aplicaciones Web**

```bash
Usuario: "Revisa los logs de error de la aplicaci√≥n e-commerce y ejecuta el diagn√≥stico de rendimiento"
LLM: [Lee logs] [Ejecuta diagn√≥stico] "Detect√© 47 errores 500 en las √∫ltimas 24h, principalmente en el checkout. El diagn√≥stico revela latencia de base de datos. He optimizado las consultas cr√≠ticas, reduciendo el tiempo de respuesta en un 40%."
```

### 4. **Gesti√≥n de Base de Datos Empresarial**

```bash
Usuario: "Analiza la base de datos de clientes y ejecuta la limpieza de datos duplicados"
LLM: [Lee DB] [Ejecuta limpieza] "Identifiqu√© 1,247 registros duplicados (8.3% del total). He consolidado los datos manteniendo la informaci√≥n m√°s reciente. La limpieza liber√≥ 156MB de espacio y mejor√≥ la integridad referencial."
```

### 5. **Seguridad y Compliance**

```bash
Usuario: "Escanea el sistema en busca de vulnerabilidades y ejecuta la actualizaci√≥n de parches cr√≠ticos"
LLM: [Lee reporte] [Ejecuta parches] "Encontr√© 3 vulnerabilidades cr√≠ticas (CVE-2024-1234, CVE-2024-5678) y 7 de nivel medio. He aplicado los parches cr√≠ticos y programado las actualizaciones de seguridad para las 2:00 AM."
```

### 6. **An√°lisis de Datos de Marketing**

```bash
Usuario: "Lee los datos de campa√±as digitales y ejecuta el an√°lisis de ROI por canal"
LLM: [Lee m√©tricas] [Ejecuta an√°lisis] "La campa√±a Q4 gener√≥ $890K en ventas con un ROI del 320%. Google Ads lidera con 45% de conversiones, mientras que Facebook muestra un costo por adquisici√≥n 23% menor. Recomiendo reasignar 15% del presupuesto a Facebook."
```

### 7. **Gesti√≥n de Contenedores y Microservicios**

```bash
Usuario: "Monitorea el estado de los contenedores en Kubernetes y ejecuta la limpieza de recursos"
LLM: [Lee estado] [Ejecuta limpieza] "Tienes 47 pods ejecut√°ndose en 8 namespaces. He identificado 12 pods hu√©rfanos y 3 deployments con 0 r√©plicas. La limpieza liber√≥ 4.2GB de almacenamiento y optimiz√≥ el uso de CPU en un 18%."
```

### 8. **An√°lisis de Logs y Troubleshooting**

```bash
Usuario: "Analiza los logs de la aplicaci√≥n de pagos y ejecuta el diagn√≥stico de transacciones fallidas"
LLM: [Lee logs] [Ejecuta diagn√≥stico] "En las √∫ltimas 6 horas, 23 transacciones fallaron (0.12% del total). El patr√≥n indica timeouts en la integraci√≥n con el procesador de pagos. He ajustado los timeouts y implementado retry autom√°tico, reduciendo fallas en un 85%."
```

### 9. **Gesti√≥n de Configuraci√≥n de Red**

```bash
Usuario: "Lee la configuraci√≥n de red y ejecuta la optimizaci√≥n de ancho de banda"
LLM: [Lee config] [Ejecuta optimizaci√≥n] "La configuraci√≥n actual tiene 30% de ancho de banda subutilizado. He optimizado el QoS, priorizado tr√°fico cr√≠tico y ajustado el rate limiting. Esto mejorar√° la latencia en un 25% y reducir√° el uso de ancho de banda en un 15%."
```

### 10. **An√°lisis de Datos de Recursos Humanos**

```bash
Usuario: "Lee los datos de rendimiento de empleados y ejecuta el an√°lisis de productividad por departamento"
LLM: [Lee datos] [Ejecuta an√°lisis] "El an√°lisis muestra que el departamento de Desarrollo tiene la mayor productividad (94%), mientras que Ventas requiere atenci√≥n con un 78%. He identificado 3 equipos que podr√≠an beneficiarse de capacitaci√≥n adicional, proyectando un incremento del 12% en productividad."
```

## ‚öõÔ∏è ¬øC√≥mo se usa en React?

Aqu√≠ te muestro c√≥mo integrar el servidor MCP con una aplicaci√≥n React usando nuestro hook personalizado:

### **1. Instalar dependencias**

```bash
npm install react @types/react @modelcontextprotocol/sdk
```

### **2. Usar el hook useMCP**

```tsx
import React, { useState } from "react";
import { useMCP } from "./hooks/useMCP";

export function MCPInterface() {
  const { isConnected, loading, readResource, callTool } = useMCP();
  const [resourceContent, setResourceContent] = useState("");
  const [toolResult, setToolResult] = useState("");

  const handleReadResource = async () => {
    const result = await readResource("file:///hello.txt");
    if (!result.error) {
      setResourceContent(result.content);
    }
  };

  const handleCallTool = async () => {
    const result = await callTool("tool-pelusear");
    if (!result.error) {
      setToolResult(result.content);
    }
  };

  return (
    <div>
      <div>Estado: {isConnected ? "‚úÖ Conectado" : "üîå Desconectado"}</div>
      <button onClick={handleReadResource} disabled={loading}>
        Leer Recurso
      </button>
      <button onClick={handleCallTool} disabled={loading}>
        Ejecutar Herramienta
      </button>

      {resourceContent && (
        <div>
          <h3>Contenido del recurso:</h3>
          <pre>{resourceContent}</pre>
        </div>
      )}

      {toolResult && (
        <div>
          <h3>Resultado de la herramienta:</h3>
          <pre>{toolResult}</pre>
        </div>
      )}
    </div>
  );
}
```

### **3. Integrar con LLM Directamente (Ollama)**

```tsx
export function LLMInterface() {
  const { isConnected, readResource, callTool } = useMCP();
  const [query, setQuery] = useState("");
  const [response, setResponse] = useState("");
  const [loading, setLoading] = useState(false);

  const callOllama = async (prompt: string) => {
    const response = await fetch("http://localhost:11434/api/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        model: "llama3.2:3b",
        messages: [
          {
            role: "system",
            content:
              'Eres un asistente que puede acceder a recursos y ejecutar herramientas. Responde solo con "leer" si quieres leer un archivo, "herramienta" si quieres ejecutar una herramienta, o "ambos" si quieres hacer las dos cosas.',
          },
          { role: "user", content: prompt },
        ],
        stream: false,
      }),
    });

    const data = await response.json();
    return data.message.content.toLowerCase();
  };

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    setLoading(true);

    try {
      // 1. Enviar consulta a Ollama
      const llmResponse = await callOllama(query);

      // 2. Ejecutar acciones en MCP basadas en la respuesta del LLM
      const results = [];

      if (llmResponse.includes("leer") || llmResponse.includes("ambos")) {
        const result = await readResource("file:///hello.txt");
        results.push({ action: "Leer archivo", result: result.content });
      }

      if (
        llmResponse.includes("herramienta") ||
        llmResponse.includes("ambos")
      ) {
        const result = await callTool("tool-pelusear");
        results.push({
          action: "Ejecutar herramienta",
          result: result.content,
        });
      }

      // 3. Generar respuesta final
      setResponse(
        `LLM dijo: "${llmResponse}". Acciones ejecutadas: ${results
          .map((r) => r.result)
          .join(", ")}`
      );
    } catch (error) {
      setResponse(
        `Error: ${error instanceof Error ? error.message : "Error desconocido"}`
      );
    } finally {
      setLoading(false);
    }
  };

  return (
    <div>
      <form onSubmit={handleSubmit}>
        <input
          type="text"
          value={query}
          onChange={(e) => setQuery(e.target.value)}
          placeholder="¬øQu√© quieres que haga?"
          disabled={loading}
        />
        <button type="submit" disabled={loading}>
          {loading ? "Procesando..." : "Enviar"}
        </button>
      </form>

      {response && (
        <div>
          <h3>Respuesta:</h3>
          <p>{response}</p>
        </div>
      )}
    </div>
  );
}
```

### **4. Archivos disponibles**

- **`hooks/useMCP.ts`** - Hook principal para conectar con MCP
- **`hooks/useMCP.example.ts`** - Ejemplos de uso
- **`hooks/README.md`** - Documentaci√≥n completa del hook

### **5. API Routes (Next.js)**

<details>
<summary>Ver ejemplo de Next.js API Route</summary>

```typescript
// pages/api/llm.ts
export default async function handler(req, res) {
  const { query } = req.body;

  const response = await fetch("http://localhost:11434/api/chat", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: "llama3.2:3b",
      messages: [
        {
          role: "system",
          content:
            "Eres un asistente que puede acceder a recursos y ejecutar herramientas.",
        },
        { role: "user", content: query },
      ],
      stream: false,
    }),
  });

  const data = await response.json();
  res.status(200).json({ content: data.message.content });
}
```

</details>

### **6. React Router v7 (Loader/Action)**

<details>
<summary>Ver ejemplo de React Router v7</summary>

```typescript
// routes/api.llm.tsx
import type { ActionFunctionArgs } from "react-router";

export async function action({ request }: ActionFunctionArgs) {
  const formData = await request.formData();
  const query = formData.get("query") as string;

  const response = await fetch("http://localhost:11434/api/chat", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: "llama3.2:3b",
      messages: [
        {
          role: "system",
          content:
            "Eres un asistente que puede acceder a recursos y ejecutar herramientas.",
        },
        { role: "user", content: query },
      ],
      stream: false,
    }),
  });

  const data = await response.json();
  return new Response(JSON.stringify({ content: data.message.content }), {
    headers: { "Content-Type": "application/json" },
  });
}

// routes/_index.tsx
import { Form } from "react-router";
import { useMCP } from "~/hooks/useMCP";

export default function Index() {
  const { readResource, callTool } = useMCP();

  return (
    <div>
      <Form method="post">
        <input type="text" name="query" placeholder="¬øQu√© quieres que haga?" />
        <button type="submit">Enviar</button>
      </Form>
    </div>
  );
}
```

</details>

### **7. Hono (API Route)**

<details>
<summary>Ver ejemplo de Hono</summary>

```typescript
// api/llm.ts
import { Hono } from "hono";

const app = new Hono();

app.post("/api/llm", async (c) => {
  const { query } = await c.req.json();

  const response = await fetch("http://localhost:11434/api/chat", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: "llama3.2:3b",
      messages: [
        {
          role: "system",
          content:
            "Eres un asistente que puede acceder a recursos y ejecutar herramientas.",
        },
        { role: "user", content: query },
      ],
      stream: false,
    }),
  });

  const data = await response.json();
  return c.json({ content: data.message.content });
});

export default app;
```

</details>

**Beneficios:**

- ‚úÖ **Hook reutilizable** para cualquier componente React
- ‚úÖ **Conexi√≥n autom√°tica** al servidor MCP
- ‚úÖ **Manejo de estados** y errores incluido
- ‚úÖ **Integraci√≥n f√°cil** con LLMs
- ‚úÖ **TypeScript** completo

## üì¶ Instalaci√≥n

```bash
npm install
```

## üöÄ Uso

### 1. Servidor MCP B√°sico

```bash
npm start
```

O en modo desarrollo (con watch):

```bash
npm run dev
```

### 2. Cliente Web Interactivo

Para usar el cliente web interactivo:

```bash
npm run web
```

Luego abre tu navegador en: http://localhost:3001

El cliente web te permite:

- Conectar/desconectar del servidor MCP
- Leer recursos (como `file:///hello.txt`)
- Ejecutar herramientas (como `tool-pelusear`)

### 3. Cliente LLM REST API

Para conectar tu LLM REST API con el servidor MCP:

```bash
# Configurar API key
export OPENAI_API_KEY="tu-api-key"
# o
export ANTHROPIC_API_KEY="tu-api-key"

# Ejecutar cliente LLM
npm run llm
```

#### Configuraci√≥n para diferentes proveedores:

**OpenAI (GPT):**

```bash
export OPENAI_API_KEY="sk-..."
npm run llm
```

**Anthropic (Claude):**

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
npm run llm
```

**Ollama (local):**

```bash
export OLLAMA_HOST="http://localhost:11434"
npm run llm
```

**API personalizada:**

```typescript
import { LLMRestClient } from "./llm_rest_client.js";
import { LLMConfigs } from "./llm_config.js";

const config = LLMConfigs.custom(
  "https://tu-api.com/v1/chat",
  "tu-modelo",
  "tu-api-key"
);

const client = new LLMRestClient(config);
```

#### Ejemplo de uso con LLM:

```
Usuario: "Lee el archivo y ejecuta la herramienta"

LLM: "Voy a leer el archivo hello.txt y luego ejecutar la herramienta de pelusear"

MCP: Lee "Hello, World!" y ejecuta herramienta que responde "¬°Has sido peluseado! üê∂"

LLM: "Perfecto! He le√≠do el archivo que contiene 'Hello, World!' y ejecut√© la herramienta que respondi√≥ '¬°Has sido peluseado! üê∂'. ¬°Ha sido una experiencia completa!"
```

## üß© Estructura del Servidor MCP

El servidor incluye:

1. **Recurso**: `file:///hello.txt` - Devuelve "Hello, World!"
2. **Herramienta**: `tool-pelusear` - Devuelve "¬°Has sido peluseado! üê∂"

### Agregar nuevos recursos:

```typescript
server.registerResource(
  "mi-recurso",
  "file:///mi-archivo.txt",
  { title: "Mi Recurso" },
  async () => ({
    contents: [
      {
        uri: "file:///mi-archivo.txt",
        text: "Contenido del archivo",
        mimeType: "text/plain",
      },
    ],
  })
);
```

### Agregar nuevas herramientas:

```typescript
server.registerTool(
  "mi-herramienta",
  {
    title: "Mi Herramienta",
    description: "Descripci√≥n de mi herramienta",
  },
  async () => ({
    content: [
      {
        type: "text",
        text: "Resultado de mi herramienta",
      },
    ],
  })
);
```

## üß™ Testing

Ejecutar los tests de integraci√≥n:

```bash
npm test
```

Los tests verifican que:

- El servidor se inicia correctamente
- Los recursos se pueden leer
- Las herramientas se pueden ejecutar
- El cliente web funciona correctamente

## üìÅ Estructura del Proyecto

```
mcp_sdk_experiment/
‚îú‚îÄ‚îÄ mcp_server.ts              # Servidor MCP principal
‚îú‚îÄ‚îÄ web_server.ts              # Servidor web + WebSocket proxy
‚îú‚îÄ‚îÄ web_client.html            # Cliente web interactivo
‚îú‚îÄ‚îÄ llm_rest_client.ts         # Cliente LLM REST API
‚îú‚îÄ‚îÄ llm_config.ts              # Configuraciones de LLM
‚îú‚îÄ‚îÄ mcp_server.integration.test.ts  # Tests de integraci√≥n
‚îú‚îÄ‚îÄ web_server.test.ts         # Tests del servidor web
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îî‚îÄ‚îÄ README.md
```

## üîß Caracter√≠sticas

- ‚úÖ **Servidor MCP b√°sico** con recursos y herramientas
- ‚úÖ **Cliente web interactivo** con WebSocket
- ‚úÖ **Cliente LLM REST API** para m√∫ltiples proveedores
- ‚úÖ **Soporte para OpenAI, Claude, Ollama** y APIs personalizadas
- ‚úÖ **Tests de integraci√≥n** completos
- ‚úÖ **TypeScript** completo con tipos
- ‚úÖ **Manejo de errores** robusto
- ‚úÖ **Documentaci√≥n** detallada

## üöÄ Pr√≥ximos Pasos

- [ ] Agregar m√°s recursos (APIs, bases de datos, etc.)
- [ ] Implementar herramientas m√°s complejas
- [ ] Agregar autenticaci√≥n y seguridad
- [ ] Mejorar la interfaz web
- [ ] Agregar m√°s tests
- [ ] Integrar con m√°s proveedores LLM
- [ ] Crear dashboard de monitoreo
- [ ] Agregar persistencia de datos
- [ ] ‚ö° Implementar streaming de respuestas

## ü§ù Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## üôè Agradecimientos

- [Model Context Protocol](https://modelcontextprotocol.io/) por el est√°ndar
- [Anthropic](https://www.anthropic.com/) por Claude
- [OpenAI](https://openai.com/) por GPT
- [Ollama](https://ollama.ai/) por el modelo local

## üß™ Ejecuci√≥n de Ejemplos

Para correr todos los ejemplos del cliente LLM (incluyendo integraci√≥n con MCP y diferentes modelos):

```bash
npm run example
```

Esto ejecutar√° el archivo `llm_client_example.ts` y mostrar√° en consola los resultados de cada ejemplo:

- Ejemplo b√°sico con Ollama
- Ejemplo con OpenAI (si tienes API key)
- Ejemplo de m√∫ltiples consultas
- Ejemplo de manejo de errores
- Ejemplo de configuraci√≥n personalizada (usa un modelo ligero por defecto)

**Recomendaci√≥n:** Para pruebas r√°pidas, usa modelos ligeros de Ollama como `llama3.2:3b` o `gemma3:4b`. Los modelos grandes como `phi4:14b` pueden ser lentos.

## üõ†Ô∏è Soluci√≥n de Problemas

- **Error 404 o 401:** Verifica que la URL y el modelo existan y que tu API key sea v√°lida.
- **Error de JSON o streaming:** El cliente fuerza `stream: false` para compatibilidad con Ollama. Si usas otro LLM, revisa el formato de respuesta.
- **Lentitud:** Usa modelos m√°s peque√±os para desarrollo. Los modelos grandes pueden tardar varios minutos.
- **Ollama no responde:** Aseg√∫rate de que Ollama est√© corriendo (`ollama serve`) y que el modelo est√© descargado (`ollama pull llama3.2:3b`).

## üìù Scripts √∫tiles

```bash
npm run start      # Inicia el servidor MCP
npm run dev        # Modo desarrollo (watch)
npm run web        # Cliente web interactivo
npm run llm        # Cliente LLM REST API
npm run example    # Ejecuta todos los ejemplos del cliente LLM
npm test           # Corre los tests de integraci√≥n
```
