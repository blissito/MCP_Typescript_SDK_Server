# MCP Server Experiment

Un servidor simple de Model Context Protocol (MCP) creado con TypeScript con soporte para LLMs REST API.

## ğŸ¯ Â¿QuÃ© es esto?

Este proyecto demuestra cÃ³mo conectar un **LLM REST API** (como OpenAI, Claude, Ollama, etc.) con un **servidor MCP** para darle "superpoderes" al LLM, permitiÃ©ndole acceder a recursos y ejecutar herramientas en tu sistema.

## ğŸ—ï¸ Arquitectura

```
Usuario â†’ LLM REST API â†’ LLMRestClient â†’ MCP Server â†’ Recursos/Herramientas
```

### Flujo de trabajo:

1. **Usuario** hace una consulta al LLM
2. **LLM** analiza quÃ© necesita hacer
3. **LLMRestClient** ejecuta las acciones en el servidor MCP
4. **MCP Server** lee recursos o ejecuta herramientas
5. **LLM** genera una respuesta final con los resultados

## ğŸ¯ Casos de Uso

### 1. **Asistente con Acceso a Archivos**

```bash
Usuario: "Lee mi archivo de configuraciÃ³n y dime quÃ© puertos estÃ¡n abiertos"
LLM: [Lee archivo] "Tu archivo muestra que tienes los puertos 3000, 8080 y 5432 abiertos"
```

### 2. **Herramientas de Sistema**

```bash
Usuario: "Ejecuta la herramienta de limpieza y luego lee el log"
LLM: [Ejecuta herramienta] [Lee log] "He limpiado el sistema y el log muestra que se eliminaron 15 archivos temporales"
```

### 3. **APIs Externas**

```bash
Usuario: "Consulta el clima y luego ejecuta la herramienta de notificaciÃ³n"
LLM: [Lee API clima] [Ejecuta notificaciÃ³n] "El clima estÃ¡ soleado a 25Â°C y he enviado la notificaciÃ³n"
```

### 4. **AnÃ¡lisis de Datos**

```bash
Usuario: "Lee el archivo CSV de ventas y ejecuta el anÃ¡lisis de tendencias"
LLM: [Lee CSV] [Ejecuta anÃ¡lisis] "Las ventas han aumentado un 23% este mes, con el producto A siendo el mÃ¡s vendido"
```

### 5. **Monitoreo de Servicios**

```bash
Usuario: "Verifica el estado de mis servicios y ejecuta la herramienta de backup"
LLM: [Lee estado servicios] [Ejecuta backup] "Todos los servicios estÃ¡n funcionando correctamente y he iniciado el backup programado"
```

### 6. **GestiÃ³n de Base de Datos**

```bash
Usuario: "Consulta la base de datos de usuarios y ejecuta la limpieza de registros duplicados"
LLM: [Lee DB] [Ejecuta limpieza] "EncontrÃ© 47 usuarios duplicados y los he consolidado correctamente"
```

### 7. **AnÃ¡lisis de Logs**

```bash
Usuario: "Lee los logs de error de hoy y ejecuta la herramienta de diagnÃ³stico"
LLM: [Lee logs] [Ejecuta diagnÃ³stico] "DetectÃ© 3 errores crÃ­ticos relacionados con la conexiÃ³n de red, he ejecutado el diagnÃ³stico y estÃ¡ todo resuelto"
```

### 8. **GestiÃ³n de ConfiguraciÃ³n**

```bash
Usuario: "Lee mi archivo de configuraciÃ³n de red y ejecuta la herramienta de optimizaciÃ³n"
LLM: [Lee config] [Ejecuta optimizaciÃ³n] "Tu configuraciÃ³n de red tiene algunos parÃ¡metros subÃ³ptimos, los he ajustado para mejorar el rendimiento en un 15%"
```

### 9. **AnÃ¡lisis de Seguridad**

```bash
Usuario: "Escanea mi sistema en busca de vulnerabilidades y ejecuta la herramienta de parcheo"
LLM: [Lee reporte] [Ejecuta parcheo] "EncontrÃ© 2 vulnerabilidades de seguridad de nivel medio, he aplicado los parches correspondientes"
```

### 10. **GestiÃ³n de Contenedores**

```bash
Usuario: "Lee el estado de mis contenedores Docker y ejecuta la herramienta de limpieza"
LLM: [Lee estado] [Ejecuta limpieza] "Tienes 5 contenedores ejecutÃ¡ndose y 3 detenidos, he limpiado los recursos no utilizados liberando 2GB de espacio"
```

## ğŸ“¦ InstalaciÃ³n

```bash
npm install
```

## ğŸš€ Uso

### 1. Servidor MCP BÃ¡sico

```bash
npm start
```

O en modo desarrollo (con watch):

```bash
npm run dev
```

### 2. Cliente Web Interactivo

Para usar el cliente web interactivo:

```bash
npm run web
```

Luego abre tu navegador en: http://localhost:3001

El cliente web te permite:

- Conectar/desconectar del servidor MCP
- Leer recursos (como `file:///hello.txt`)
- Ejecutar herramientas (como `tool-pelusear`)

### 3. Cliente LLM REST API

Para conectar tu LLM REST API con el servidor MCP:

```bash
# Configurar API key
export OPENAI_API_KEY="tu-api-key"
# o
export ANTHROPIC_API_KEY="tu-api-key"

# Ejecutar cliente LLM
npm run llm
```

#### ConfiguraciÃ³n para diferentes proveedores:

**OpenAI (GPT):**

```bash
export OPENAI_API_KEY="sk-..."
npm run llm
```

**Anthropic (Claude):**

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
npm run llm
```

**Ollama (local):**

```bash
export OLLAMA_HOST="http://localhost:11434"
npm run llm
```

**API personalizada:**

```typescript
import { LLMRestClient } from "./llm_rest_client.js";
import { LLMConfigs } from "./llm_config.js";

const config = LLMConfigs.custom(
  "https://tu-api.com/v1/chat",
  "tu-modelo",
  "tu-api-key"
);

const client = new LLMRestClient(config);
```

#### Ejemplo de uso con LLM:

```
Usuario: "Lee el archivo y ejecuta la herramienta"

LLM: "Voy a leer el archivo hello.txt y luego ejecutar la herramienta de pelusear"

MCP: Lee "Hello, World!" y ejecuta herramienta que responde "Â¡Has sido peluseado! ğŸ¶"

LLM: "Perfecto! He leÃ­do el archivo que contiene 'Hello, World!' y ejecutÃ© la herramienta que respondiÃ³ 'Â¡Has sido peluseado! ğŸ¶'. Â¡Ha sido una experiencia completa!"
```

## ğŸ§© Estructura del Servidor MCP

El servidor incluye:

1. **Recurso**: `file:///hello.txt` - Devuelve "Hello, World!"
2. **Herramienta**: `tool-pelusear` - Devuelve "Â¡Has sido peluseado! ğŸ¶"

### Agregar nuevos recursos:

```typescript
server.registerResource(
  "mi-recurso",
  "file:///mi-archivo.txt",
  { title: "Mi Recurso" },
  async () => ({
    contents: [
      {
        uri: "file:///mi-archivo.txt",
        text: "Contenido del archivo",
        mimeType: "text/plain",
      },
    ],
  })
);
```

### Agregar nuevas herramientas:

```typescript
server.registerTool(
  "mi-herramienta",
  {
    title: "Mi Herramienta",
    description: "DescripciÃ³n de mi herramienta",
  },
  async () => ({
    content: [
      {
        type: "text",
        text: "Resultado de mi herramienta",
      },
    ],
  })
);
```

## ğŸ§ª Testing

Ejecutar los tests de integraciÃ³n:

```bash
npm test
```

Los tests verifican que:

- El servidor se inicia correctamente
- Los recursos se pueden leer
- Las herramientas se pueden ejecutar
- El cliente web funciona correctamente

## ğŸ“ Estructura del Proyecto

```
mcp_sdk_experiment/
â”œâ”€â”€ mcp_server.ts              # Servidor MCP principal
â”œâ”€â”€ web_server.ts              # Servidor web + WebSocket proxy
â”œâ”€â”€ web_client.html            # Cliente web interactivo
â”œâ”€â”€ llm_rest_client.ts         # Cliente LLM REST API
â”œâ”€â”€ llm_config.ts              # Configuraciones de LLM
â”œâ”€â”€ mcp_server.integration.test.ts  # Tests de integraciÃ³n
â”œâ”€â”€ web_server.test.ts         # Tests del servidor web
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

## ğŸ”§ CaracterÃ­sticas

- âœ… **Servidor MCP bÃ¡sico** con recursos y herramientas
- âœ… **Cliente web interactivo** con WebSocket
- âœ… **Cliente LLM REST API** para mÃºltiples proveedores
- âœ… **Soporte para OpenAI, Claude, Ollama** y APIs personalizadas
- âœ… **Tests de integraciÃ³n** completos
- âœ… **TypeScript** completo con tipos
- âœ… **Manejo de errores** robusto
- âœ… **DocumentaciÃ³n** detallada

## ğŸš€ PrÃ³ximos Pasos

- [ ] Agregar mÃ¡s recursos (APIs, bases de datos, etc.)
- [ ] Implementar herramientas mÃ¡s complejas
- [ ] Agregar autenticaciÃ³n y seguridad
- [ ] Mejorar la interfaz web
- [ ] Agregar mÃ¡s tests
- [ ] Integrar con mÃ¡s proveedores LLM
- [ ] Crear dashboard de monitoreo
- [ ] Agregar persistencia de datos
- [ ] âš¡ Implementar streaming de respuestas

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## ğŸ™ Agradecimientos

- [Model Context Protocol](https://modelcontextprotocol.io/) por el estÃ¡ndar
- [Anthropic](https://www.anthropic.com/) por Claude
- [OpenAI](https://openai.com/) por GPT
- [Ollama](https://ollama.ai/) por el modelo local
