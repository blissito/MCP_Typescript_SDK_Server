# MCP Server Experiment

Un servidor simple de Model Context Protocol (MCP) creado con TypeScript con soporte para LLMs REST API.

## ğŸ¯ Â¿QuÃ© es esto?

Este proyecto demuestra cÃ³mo conectar un **LLM REST API** (como OpenAI, Claude, Ollama, etc.) con un **servidor MCP** para darle "superpoderes" al LLM, permitiÃ©ndole acceder a recursos y ejecutar herramientas en tu sistema.

## ğŸ—ï¸ Arquitectura

```
Usuario â†’ LLM REST API â†’ LLMRestClient â†’ MCP Server â†’ Recursos/Herramientas
```

### Flujo de trabajo:

1. **Usuario** hace una consulta al LLM
2. **LLM** analiza quÃ© necesita hacer
3. **LLMRestClient** ejecuta las acciones en el servidor MCP
4. **MCP Server** lee recursos o ejecuta herramientas
5. **LLM** genera una respuesta final con los resultados

## ğŸ¯ Casos de Uso

### 1. **GestiÃ³n de Infraestructura IT**

```bash
Usuario: "Verifica el estado de los servidores de producciÃ³n y ejecuta el backup automÃ¡tico"
LLM: [Lee mÃ©tricas] [Ejecuta backup] "Los 12 servidores estÃ¡n operativos al 99.2% de rendimiento. He iniciado el backup incremental que completarÃ¡ en 15 minutos, liberando 2.3GB de espacio."
```

### 2. **AnÃ¡lisis Financiero y Reportes**

```bash
Usuario: "Lee el reporte de ventas del Q4 y ejecuta el anÃ¡lisis de rentabilidad por regiÃ³n"
LLM: [Lee datos] [Ejecuta anÃ¡lisis] "Las ventas del Q4 alcanzaron $2.4M, un incremento del 18% vs Q3. La regiÃ³n Norte muestra la mayor rentabilidad (34%), mientras que el Sur requiere atenciÃ³n con un margen del 12%."
```

### 3. **Monitoreo de Aplicaciones Web**

```bash
Usuario: "Revisa los logs de error de la aplicaciÃ³n e-commerce y ejecuta el diagnÃ³stico de rendimiento"
LLM: [Lee logs] [Ejecuta diagnÃ³stico] "DetectÃ© 47 errores 500 en las Ãºltimas 24h, principalmente en el checkout. El diagnÃ³stico revela latencia de base de datos. He optimizado las consultas crÃ­ticas, reduciendo el tiempo de respuesta en un 40%."
```

### 4. **GestiÃ³n de Base de Datos Empresarial**

```bash
Usuario: "Analiza la base de datos de clientes y ejecuta la limpieza de datos duplicados"
LLM: [Lee DB] [Ejecuta limpieza] "IdentifiquÃ© 1,247 registros duplicados (8.3% del total). He consolidado los datos manteniendo la informaciÃ³n mÃ¡s reciente. La limpieza liberÃ³ 156MB de espacio y mejorÃ³ la integridad referencial."
```

### 5. **Seguridad y Compliance**

```bash
Usuario: "Escanea el sistema en busca de vulnerabilidades y ejecuta la actualizaciÃ³n de parches crÃ­ticos"
LLM: [Lee reporte] [Ejecuta parches] "EncontrÃ© 3 vulnerabilidades crÃ­ticas (CVE-2024-1234, CVE-2024-5678) y 7 de nivel medio. He aplicado los parches crÃ­ticos y programado las actualizaciones de seguridad para las 2:00 AM."
```

### 6. **AnÃ¡lisis de Datos de Marketing**

```bash
Usuario: "Lee los datos de campaÃ±as digitales y ejecuta el anÃ¡lisis de ROI por canal"
LLM: [Lee mÃ©tricas] [Ejecuta anÃ¡lisis] "La campaÃ±a Q4 generÃ³ $890K en ventas con un ROI del 320%. Google Ads lidera con 45% de conversiones, mientras que Facebook muestra un costo por adquisiciÃ³n 23% menor. Recomiendo reasignar 15% del presupuesto a Facebook."
```

### 7. **GestiÃ³n de Contenedores y Microservicios**

```bash
Usuario: "Monitorea el estado de los contenedores en Kubernetes y ejecuta la limpieza de recursos"
LLM: [Lee estado] [Ejecuta limpieza] "Tienes 47 pods ejecutÃ¡ndose en 8 namespaces. He identificado 12 pods huÃ©rfanos y 3 deployments con 0 rÃ©plicas. La limpieza liberÃ³ 4.2GB de almacenamiento y optimizÃ³ el uso de CPU en un 18%."
```

### 8. **AnÃ¡lisis de Logs y Troubleshooting**

```bash
Usuario: "Analiza los logs de la aplicaciÃ³n de pagos y ejecuta el diagnÃ³stico de transacciones fallidas"
LLM: [Lee logs] [Ejecuta diagnÃ³stico] "En las Ãºltimas 6 horas, 23 transacciones fallaron (0.12% del total). El patrÃ³n indica timeouts en la integraciÃ³n con el procesador de pagos. He ajustado los timeouts y implementado retry automÃ¡tico, reduciendo fallas en un 85%."
```

### 9. **GestiÃ³n de ConfiguraciÃ³n de Red**

```bash
Usuario: "Lee la configuraciÃ³n de red y ejecuta la optimizaciÃ³n de ancho de banda"
LLM: [Lee config] [Ejecuta optimizaciÃ³n] "La configuraciÃ³n actual tiene 30% de ancho de banda subutilizado. He optimizado el QoS, priorizado trÃ¡fico crÃ­tico y ajustado el rate limiting. Esto mejorarÃ¡ la latencia en un 25% y reducirÃ¡ el uso de ancho de banda en un 15%."
```

### 10. **AnÃ¡lisis de Datos de Recursos Humanos**

```bash
Usuario: "Lee los datos de rendimiento de empleados y ejecuta el anÃ¡lisis de productividad por departamento"
LLM: [Lee datos] [Ejecuta anÃ¡lisis] "El anÃ¡lisis muestra que el departamento de Desarrollo tiene la mayor productividad (94%), mientras que Ventas requiere atenciÃ³n con un 78%. He identificado 3 equipos que podrÃ­an beneficiarse de capacitaciÃ³n adicional, proyectando un incremento del 12% en productividad."
```

## ğŸ“¦ InstalaciÃ³n

```bash
npm install
```

## ğŸš€ Uso

### 1. Servidor MCP BÃ¡sico

```bash
npm start
```

O en modo desarrollo (con watch):

```bash
npm run dev
```

### 2. Cliente Web Interactivo

Para usar el cliente web interactivo:

```bash
npm run web
```

Luego abre tu navegador en: http://localhost:3001

El cliente web te permite:

- Conectar/desconectar del servidor MCP
- Leer recursos (como `file:///hello.txt`)
- Ejecutar herramientas (como `tool-pelusear`)

### 3. Cliente LLM REST API

Para conectar tu LLM REST API con el servidor MCP:

```bash
# Configurar API key
export OPENAI_API_KEY="tu-api-key"
# o
export ANTHROPIC_API_KEY="tu-api-key"

# Ejecutar cliente LLM
npm run llm
```

#### ConfiguraciÃ³n para diferentes proveedores:

**OpenAI (GPT):**

```bash
export OPENAI_API_KEY="sk-..."
npm run llm
```

**Anthropic (Claude):**

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
npm run llm
```

**Ollama (local):**

```bash
export OLLAMA_HOST="http://localhost:11434"
npm run llm
```

**API personalizada:**

```typescript
import { LLMRestClient } from "./llm_rest_client.js";
import { LLMConfigs } from "./llm_config.js";

const config = LLMConfigs.custom(
  "https://tu-api.com/v1/chat",
  "tu-modelo",
  "tu-api-key"
);

const client = new LLMRestClient(config);
```

#### Ejemplo de uso con LLM:

```
Usuario: "Lee el archivo y ejecuta la herramienta"

LLM: "Voy a leer el archivo hello.txt y luego ejecutar la herramienta de pelusear"

MCP: Lee "Hello, World!" y ejecuta herramienta que responde "Â¡Has sido peluseado! ğŸ¶"

LLM: "Perfecto! He leÃ­do el archivo que contiene 'Hello, World!' y ejecutÃ© la herramienta que respondiÃ³ 'Â¡Has sido peluseado! ğŸ¶'. Â¡Ha sido una experiencia completa!"
```

## ğŸ§© Estructura del Servidor MCP

El servidor incluye:

1. **Recurso**: `file:///hello.txt` - Devuelve "Hello, World!"
2. **Herramienta**: `tool-pelusear` - Devuelve "Â¡Has sido peluseado! ğŸ¶"

### Agregar nuevos recursos:

```typescript
server.registerResource(
  "mi-recurso",
  "file:///mi-archivo.txt",
  { title: "Mi Recurso" },
  async () => ({
    contents: [
      {
        uri: "file:///mi-archivo.txt",
        text: "Contenido del archivo",
        mimeType: "text/plain",
      },
    ],
  })
);
```

### Agregar nuevas herramientas:

```typescript
server.registerTool(
  "mi-herramienta",
  {
    title: "Mi Herramienta",
    description: "DescripciÃ³n de mi herramienta",
  },
  async () => ({
    content: [
      {
        type: "text",
        text: "Resultado de mi herramienta",
      },
    ],
  })
);
```

## ğŸ§ª Testing

Ejecutar los tests de integraciÃ³n:

```bash
npm test
```

Los tests verifican que:

- El servidor se inicia correctamente
- Los recursos se pueden leer
- Las herramientas se pueden ejecutar
- El cliente web funciona correctamente

## ğŸ“ Estructura del Proyecto

```
mcp_sdk_experiment/
â”œâ”€â”€ mcp_server.ts              # Servidor MCP principal
â”œâ”€â”€ web_server.ts              # Servidor web + WebSocket proxy
â”œâ”€â”€ web_client.html            # Cliente web interactivo
â”œâ”€â”€ llm_rest_client.ts         # Cliente LLM REST API
â”œâ”€â”€ llm_config.ts              # Configuraciones de LLM
â”œâ”€â”€ mcp_server.integration.test.ts  # Tests de integraciÃ³n
â”œâ”€â”€ web_server.test.ts         # Tests del servidor web
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

## ğŸ”§ CaracterÃ­sticas

- âœ… **Servidor MCP bÃ¡sico** con recursos y herramientas
- âœ… **Cliente web interactivo** con WebSocket
- âœ… **Cliente LLM REST API** para mÃºltiples proveedores
- âœ… **Soporte para OpenAI, Claude, Ollama** y APIs personalizadas
- âœ… **Tests de integraciÃ³n** completos
- âœ… **TypeScript** completo con tipos
- âœ… **Manejo de errores** robusto
- âœ… **DocumentaciÃ³n** detallada

## ğŸš€ PrÃ³ximos Pasos

- [ ] Agregar mÃ¡s recursos (APIs, bases de datos, etc.)
- [ ] Implementar herramientas mÃ¡s complejas
- [ ] Agregar autenticaciÃ³n y seguridad
- [ ] Mejorar la interfaz web
- [ ] Agregar mÃ¡s tests
- [ ] Integrar con mÃ¡s proveedores LLM
- [ ] Crear dashboard de monitoreo
- [ ] Agregar persistencia de datos
- [ ] âš¡ Implementar streaming de respuestas

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para detalles.

## ğŸ™ Agradecimientos

- [Model Context Protocol](https://modelcontextprotocol.io/) por el estÃ¡ndar
- [Anthropic](https://www.anthropic.com/) por Claude
- [OpenAI](https://openai.com/) por GPT
- [Ollama](https://ollama.ai/) por el modelo local
