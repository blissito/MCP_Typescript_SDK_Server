{"version":3,"sources":["../src/client.ts"],"names":[],"mappings":";;;AAEO,IAAM,YAAN,MAAgB;AAAA,EACb,MAAA;AAAA,EAER,YAAY,MAAA,EAAmB;AAC7B,IAAA,IAAA,CAAK,MAAA,GAAS,MAAA;AAAA;AAChB,EAEA,MAAM,KAAK,QAAA,EAAoD;AAC7D,IAAA,MAAM,OAAA,GAAkC;AAAA,MACtC,cAAA,EAAgB,kBAAA;AAAA,MAChB,GAAG,KAAK,MAAA,CAAO;AAAA,KACjB;AAEA,IAAA,IAAI,IAAA,CAAK,OAAO,MAAA,EAAQ;AACtB,MAAA,OAAA,CAAQ,eAAe,CAAA,GAAI,CAAA,OAAA,EAAU,IAAA,CAAK,OAAO,MAAM,CAAA,CAAA;AAAA;AAGzD,IAAA,MAAM,QAAA,GAAW,MAAM,KAAA,CAAM,IAAA,CAAK,OAAO,GAAA,EAAK;AAAA,MAC5C,MAAA,EAAQ,MAAA;AAAA,MACR,OAAA;AAAA,MACA,IAAA,EAAM,KAAK,SAAA,CAAU;AAAA,QACnB,KAAA,EAAO,KAAK,MAAA,CAAO,KAAA;AAAA,QACnB,QAAA;AAAA,QACA,MAAA,EAAQ;AAAA,OACT;AAAA,KACF,CAAA;AAED,IAAA,IAAI,CAAC,SAAS,EAAA,EAAI;AAChB,MAAA,MAAM,IAAI,KAAA,CAAM,CAAA,oBAAA,EAAuB,QAAA,CAAS,MAAM,CAAA,CAAE,CAAA;AAAA;AAG1D,IAAA,OAAO,MAAM,SAAS,IAAA,EAAK;AAAA;AAE/B;AAGO,IAAM,kBAAA,GAAqB,CAAC,KAAA,GAAQ,aAAA,KAAkB;AAC3D,EAAA,OAAO,IAAI,SAAA,CAAU;AAAA,IACnB,GAAA,EAAK,iCAAA;AAAA,IACL;AAAA,GACD,CAAA;AACH;AAEO,IAAM,kBAAA,GAAqB,CAAC,MAAA,EAAgB,KAAA,GAAQ,eAAA,KAAoB;AAC7E,EAAA,OAAO,IAAI,SAAA,CAAU;AAAA,IACnB,GAAA,EAAK,4CAAA;AAAA,IACL,KAAA;AAAA,IACA;AAAA,GACD,CAAA;AACH;AAEO,IAAM,qBAAA,GAAwB,CACnC,MAAA,EACA,KAAA,GAAQ,0BAAA,KACL;AACH,EAAA,OAAO,IAAI,SAAA,CAAU;AAAA,IACnB,GAAA,EAAK,uCAAA;AAAA,IACL,KAAA;AAAA,IACA,MAAA;AAAA,IACA,OAAA,EAAS;AAAA,MACP,mBAAA,EAAqB;AAAA;AACvB,GACD,CAAA;AACH","file":"client.cjs","sourcesContent":["import type { LLMConfig } from \"./types\";\n\nexport class LLMClient {\n  private config: LLMConfig;\n\n  constructor(config: LLMConfig) {\n    this.config = config;\n  }\n\n  async chat(messages: Array<{ role: string; content: string }>) {\n    const headers: Record<string, string> = {\n      \"Content-Type\": \"application/json\",\n      ...this.config.headers,\n    };\n\n    if (this.config.apiKey) {\n      headers[\"Authorization\"] = `Bearer ${this.config.apiKey}`;\n    }\n\n    const response = await fetch(this.config.url, {\n      method: \"POST\",\n      headers,\n      body: JSON.stringify({\n        model: this.config.model,\n        messages,\n        stream: false,\n      }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`);\n    }\n\n    return await response.json();\n  }\n}\n\n// Pre-configured clients for common providers\nexport const createOllamaClient = (model = \"llama3.2:3b\") => {\n  return new LLMClient({\n    url: \"http://localhost:11434/api/chat\",\n    model,\n  });\n};\n\nexport const createOpenAIClient = (apiKey: string, model = \"gpt-3.5-turbo\") => {\n  return new LLMClient({\n    url: \"https://api.openai.com/v1/chat/completions\",\n    model,\n    apiKey,\n  });\n};\n\nexport const createAnthropicClient = (\n  apiKey: string,\n  model = \"claude-3-sonnet-20240229\"\n) => {\n  return new LLMClient({\n    url: \"https://api.anthropic.com/v1/messages\",\n    model,\n    apiKey,\n    headers: {\n      \"anthropic-version\": \"2023-06-01\",\n    },\n  });\n};\n"]}